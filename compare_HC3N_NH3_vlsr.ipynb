{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1488a32d-d231-48b8-8692-70db2434c32c",
   "metadata": {},
   "source": [
    "# Comparison between HC$_3$N and NH$_3$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6d70ee-8f3d-4436-b1d8-fbe9d001cdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# original resolution: 4.07672 arcsec x 3.78778 arcsec, pa: -81.4035 deg \n",
    "# matching kernel: 4.65325 arcsec x 4.40231 arcsec, pa: 8.59652 deg \n",
    "# target resolution: 6 arcsec x 6 arcsec, pa: 0 deg\n",
    "import numpy as np\n",
    "from astropy.modeling.models import Gaussian2D\n",
    "import astropy.units as u\n",
    "from astropy.nddata import Cutout2D\n",
    "from astropy.convolution import convolve\n",
    "from astropy.io import fits\n",
    "from astropy.wcs import WCS\n",
    "from astropy.coordinates import SkyCoord\n",
    "from photutils.psf import create_matching_kernel, TopHatWindow\n",
    "import matplotlib.pyplot as plt\n",
    "from reproject import reproject_exact\n",
    "import aplpy\n",
    "from B5setup import *\n",
    "from scipy import stats\n",
    "from scipy.integrate import simpson\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f906c760-f62e-4410-a80d-ce4ede3b6b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "folderHC3N = 'B5_NOEMA_30m/gaussfit/'\n",
    "maskfile = folderHC3N + 'B5-NOEMA+30m-H3CN-10-9_cut_K_mask'\n",
    "paramscubeNH3 = 'B5_previous_data/B5_parameter_maps_snr7'\n",
    "# for velocity:\n",
    "filenameHC3Nvlsr = folderHC3N + 'B5-NOEMA+30m-H3CN-10-9_cut_K_1G_fitparams_filtered_Vlsr'\n",
    "filenameHC3Nvlsr_u = folderHC3N + 'B5-NOEMA+30m-H3CN-10-9_cut_K_1G_fitparams_filtered_Vlsr_unc'\n",
    "filenameNH3vlsr = 'B5_previous_data/B5_VLA_GBT_model_vc_QA'\n",
    "diffdatafile = 'B5_Vlsr_HC3N_minus_NH3'\n",
    "diffdatafile_unc = 'B5_Vlsr_HC3N_minus_NH3_unc'\n",
    "paramscube_vlsrindex = 4\n",
    "\n",
    "# For Velocity dispersion :\n",
    "# filenameHC3Nvlsr = folderHC3N + 'B5-NOEMA+30m-H3CN-10-9_cut_K_1G_fitparams_filtered_SigmaV'\n",
    "# filenameNH3vlsr = 'B5_previous_data/B5_VLA_GBT_model_dv_QA'\n",
    "# diffdatafile = 'B5_SigmaV_HC3N_minus_NH3'\n",
    "# paramscube_sigmaindex = 3\n",
    "\n",
    "filename_convolved = filenameHC3Nvlsr + '_conv_NH3'\n",
    "filename_unc_convolved = filenameHC3Nvlsr_u + '_conv_NH3'\n",
    "filenameNH3_reproject = filenameNH3vlsr + '_reprojectHC3N'\n",
    "filenameNH3_unc_reproject = filenameNH3vlsr + '_unc_reprojectHC3N'\n",
    "\n",
    "# alma blue streamer\n",
    "filenameh2costreamer = 'B5_IRS1_ALMA/gaussfit_H2CO/analysis_central_chans_masked/components_blueshifted_envelope_vlsr'\n",
    "\n",
    "#to save the minimum points and cuts\n",
    "min_diff_HC3N_NH3_coords_file = 'min_diff_HC3N_NH3_coords.npy'\n",
    "hor_cut_file = 'horizontal_cut_HC3N_NH3_diff.npy'\n",
    "ver_cut_file = 'vertical_cut_HC3N_NH3_diff.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356a476f-cee0-47dd-83cb-bb141d9fed16",
   "metadata": {},
   "outputs": [],
   "source": [
    "headerHC3N = fits.getheader(filenameHC3Nvlsr+'.fits')\n",
    "headerNH3 = fits.getheader(filenameNH3vlsr+'.fits')\n",
    "wcsHC3N = WCS(headerHC3N)\n",
    "pixsizeHC3N = np.abs(headerHC3N['CDELT2']) # pixel size in degrees\n",
    "pixsizeNH3 = np.abs(headerNH3['CDELT2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a19c26b-045f-4a07-80e6-a68275ffced7",
   "metadata": {},
   "source": [
    "We first need to convolve the HC$_3$N to the resolution of NH$_3$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b049a0-1742-4dc2-9e14-7f370b1736df",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(filename_convolved + '.fits') or not os.path.exists(filename_unc_convolved + '.fits'):\n",
    "# we first need to transform all angle units to pixels\n",
    "    kernelsize = 29\n",
    "    xcent = int(round(kernelsize/2))\n",
    "\n",
    "    bmaj_original = headerHC3N['BMAJ']\n",
    "    bmin_original = headerHC3N['BMIN']\n",
    "    bpa_original = headerHC3N['BPA'] * u.deg\n",
    "    sigmamaj_original_pix = (bmaj_original/pixsizeHC3N) / np.sqrt(8*np.log(2))\n",
    "    sigmamin_original_pix = (bmin_original/pixsizeHC3N) / np.sqrt(8*np.log(2))\n",
    "    bmaj_target = headerNH3['BMAJ']\n",
    "    bmin_target = headerNH3['BMIN']\n",
    "    bpa_target = headerNH3['BPA'] * u.deg\n",
    "    sigmamaj_target_pix = (bmaj_target/pixsizeHC3N) / np.sqrt(8*np.log(2))\n",
    "    sigmamin_target_pix = (bmin_target/pixsizeHC3N) / np.sqrt(8*np.log(2))\n",
    "    \n",
    "    y, x = np.mgrid[0:kernelsize, 0:kernelsize]\n",
    "    beamoriginalg = Gaussian2D(100, xcent, xcent, sigmamaj_original_pix, sigmamin_original_pix, theta=bpa_original)\n",
    "    beamtargetg = Gaussian2D(100, xcent, xcent, sigmamaj_target_pix, sigmamin_target_pix, theta=bpa_target)\n",
    "    beamoriginal = beamoriginalg(x, y)\n",
    "    beamtarget = beamtargetg(x, y)\n",
    "    beamtarget /= np.sum(beamtarget)\n",
    "    window = TopHatWindow(beta=0.45)\n",
    "    matchingkernel = create_matching_kernel(beamoriginal, beamtarget, window=window)\n",
    "    \n",
    "    mask = fits.getdata(maskfile+'.fits')\n",
    "    if not os.path.exists(filename_convolved + '.fits'):\n",
    "        HC3Nvlsr = fits.getdata(filenameHC3Nvlsr+'.fits') \n",
    "    \n",
    "        HC3Nvlsrconvolved = convolve(HC3Nvlsr, matchingkernel)\n",
    "        HC3Nvlsrconvolved = np.where(mask, HC3Nvlsrconvolved, np.nan)\n",
    "\n",
    "        newheaderHC3N = headerHC3N.copy()\n",
    "        newheaderHC3N['BMAJ'] = bmaj_target\n",
    "        newheaderHC3N['BMIN'] = bmin_target\n",
    "        newheaderHC3N['BPA'] = bpa_target.value\n",
    "    \n",
    "        fits.writeto(filename_convolved+'.fits', HC3Nvlsrconvolved, newheaderHC3N)\n",
    "        \n",
    "    if not os.path.exists(filename_unc_convolved + '.fits'):\n",
    "        HC3Nvlsr_u = fits.getdata(filenameHC3Nvlsr_u+'.fits')\n",
    "        aprox_sigma_kernel = np.sqrt(sigmamaj_target_pix -sigmamaj_original_pix)\n",
    "        \n",
    "        matchingkernel_unc_factor = (2 * np.sqrt(np.pi) * sigmamaj_original_pix * aprox_sigma_kernel / np.sqrt(aprox_sigma_kernel**2 + sigmamaj_original_pix**2))\n",
    "        matchingkernel_unc = matchingkernel ** 2 * matchingkernel_unc_factor\n",
    "        HC3Nvlsr_uconvolved = convolve(HC3Nvlsr_u, matchingkernel_unc)\n",
    "        HC3Nvlsr_uconvolved = np.where(mask, HC3Nvlsr_uconvolved, np.nan)\n",
    "\n",
    "        newheaderHC3N = headerHC3N.copy()\n",
    "        newheaderHC3N['BMAJ'] = bmaj_target\n",
    "        newheaderHC3N['BMIN'] = bmin_target\n",
    "        newheaderHC3N['BPA'] = bpa_target.value\n",
    "    \n",
    "        fits.writeto(filename_unc_convolved+'.fits', HC3Nvlsr_uconvolved, newheaderHC3N)\n",
    "    HC3Nvlsrconvolved = fits.getdata(filename_convolved + '.fits')\n",
    "    HC3Nvlsr_uconvolved = fits.getdata(filename_unc_convolved + '.fits')\n",
    "else:\n",
    "    HC3Nvlsrconvolved, newheaderHC3N = fits.getdata(filename_convolved + '.fits', header=True)\n",
    "    HC3Nvlsr_uconvolved = fits.getdata(filename_unc_convolved + '.fits')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576e8ba7-a952-41f5-bfaf-e745a34e4856",
   "metadata": {},
   "source": [
    "Now we need to regrid the NH$_3$ image to the HC$_3$N grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58354d94-d657-41c1-896d-c36c57a6b077",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(filenameNH3_reproject+'.fits'):\n",
    "    NH3hdu = fits.open(filenameNH3vlsr + '.fits')\n",
    "    key_remove = ['PC03_01', 'PC04_01', 'PC01_03', 'PC02_03', 'PC03_02', 'PC04_02','PC03_03', 'PC04_03', 'PC01_04',\n",
    "                  'PC02_04', 'PC03_04', 'PC04_04', 'CUNIT3', 'CRPIX4','CDELT4','CUNIT4','CTYPE4','CRVAL4','SPECSYS']\n",
    "    for key_i in key_remove: # there are more dimensions written in the NH3 fits file\n",
    "        NH3hdu[0].header.remove(key_i)\n",
    "    newNH3image, footprint = reproject_exact(NH3hdu[0], newheaderHC3N)\n",
    "    # now we modify the spatial properties of the NH3 Header\n",
    "    newheaderNH3 = NH3hdu[0].header.copy() # we use this header as it has the resolution\n",
    "    key_modify = ['CUNIT1', 'CRPIX1','CDELT1','CTYPE1','CRVAL1', 'CUNIT2', 'CRPIX2','CDELT2','CTYPE2','CRVAL2']\n",
    "    for key_i in key_modify: # there are more dimensions written in the NH3 fits file\n",
    "        newheaderNH3[key_i] = newheaderHC3N[key_i]\n",
    "\n",
    "    fits.writeto(filenameNH3_reproject+'.fits', newNH3image, newheaderNH3)\n",
    "    NH3hdu.close()\n",
    "else:\n",
    "    newNH3image,  newheaderNH3 = fits.getdata(filenameNH3_reproject+'.fits', header=True)\n",
    "    \n",
    "if not os.path.exists(filenameNH3_unc_reproject+'.fits'):\n",
    "    params = fits.getdata(paramscubeNH3+'.fits')\n",
    "    vlsrunc = params[paramscube_vlsrindex+5]\n",
    "    header = headerNH3.copy()\n",
    "    key_remove = ['PC03_01', 'PC04_01', 'PC01_03', 'PC02_03', 'PC03_02', 'PC04_02','PC03_03', 'PC04_03', 'PC01_04',\n",
    "                  'PC02_04', 'PC03_04', 'PC04_04', 'CUNIT3', 'CRPIX4','CDELT4','CUNIT4','CTYPE4','CRVAL4','SPECSYS']\n",
    "    for key_i in key_remove: # there are more dimensions written in the NH3 fits file\n",
    "        header.remove(key_i)\n",
    "    newNH3uncimage, footprint = reproject_exact(fits.PrimaryHDU(vlsrunc, header), newheaderHC3N)\n",
    "    newheaderNH3unc = header.copy() # we use this header as it has the resolution\n",
    "    key_modify = ['CUNIT1', 'CRPIX1','CDELT1','CTYPE1','CRVAL1', 'CUNIT2', 'CRPIX2','CDELT2','CTYPE2','CRVAL2']\n",
    "    for key_i in key_modify: # there are more dimensions written in the NH3 fits file\n",
    "        newheaderNH3[key_i] = newheaderHC3N[key_i]\n",
    "    fits.writeto(filenameNH3_unc_reproject+'.fits', newNH3uncimage, newheaderNH3unc)\n",
    "else:\n",
    "    newNH3uncimage, newheaderNH3unc = fits.getdata(filenameNH3_unc_reproject+'.fits', header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce3f6c8-3291-4c06-a5f7-57c9a5b534f1",
   "metadata": {},
   "source": [
    "We look at the difference of HC$_3$N with respect to NH$_3$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318d9fc6-c160-4783-b250-56f152629f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = HC3Nvlsrconvolved - newNH3image\n",
    "newhead = newheaderHC3N.copy()\n",
    "newhead.remove('BUNIT')\n",
    "if not os.path.exists(diffdatafile + '.fits'):\n",
    "    fits.writeto(diffdatafile + '.fits', diff, newhead)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8ad670-2a00-436f-b4da-5ed94ab236d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we plot quickly the difference\n",
    "fig = plt.figure(figsize=(4,4))\n",
    "velmin = -0.2\n",
    "velmax = 0.2\n",
    "ax = plot_aplpy_subfig(diffdatafile + '.fits', \n",
    "                       fig, (1,1,1), 'linear', velmin, \n",
    "                       velmax, 'RdYlBu_r')\n",
    "ax.add_label(0.6, 0.04, \n",
    "             r'$V_{\\mathrm{LSR},\\mathrm{HC}_3\\mathrm{N}(10-9)} - V_{\\mathrm{LSR}, \\mathrm{NH}_3(1,1)}$', \n",
    "             relative=True, family='sans-serif', size=12)\n",
    "# fig.savefig(folderHC3N + diffdatafile + '.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25e13bc-5cdf-4d82-ac3c-803aed9a75d0",
   "metadata": {},
   "source": [
    "We take a quick look at the uncertainty in the subtraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1978689-9538-4158-87a0-ca40241503ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_unc = newNH3uncimage + HC3Nvlsr_uconvolved\n",
    "if not os.path.exists(diffdatafile_unc + '.fits'):\n",
    "    fits.writeto(diffdatafile_unc + '.fits', diff_unc, newhead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c113fa62-6f61-4e74-a89a-9fcb2ccd9aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we plot quickly the difference\n",
    "fig = plt.figure(figsize=(4,4))\n",
    "velmin = 0\n",
    "velmax = 0.1\n",
    "ax = plot_aplpy_subfig(diffdatafile_unc + '.fits', \n",
    "                       fig, (1,1,1), 'linear', velmin, \n",
    "                       velmax, 'RdYlBu_r')\n",
    "ax.add_label(0.6, 0.04, \n",
    "             r'$V_{\\mathrm{LSR},\\mathrm{HC}_3\\mathrm{N}(10-9)} - V_{\\mathrm{LSR}, \\mathrm{NH}_3(1,1)}$', \n",
    "             relative=True, family='sans-serif', size=12)\n",
    "ax.colorbar.set_axis_label_text('Uncertainty (km/s)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c08921-54d1-489e-8f48-c2ef17d35318",
   "metadata": {},
   "source": [
    "Now we do the KDE of the velocity difference distribution, first in 1 dimension and then in 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aebead7-f859-4259-9821-09b1924b526f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the grid for the 1D kernel distribution\n",
    "if not os.path.exists(diffdatafile+'.npy'):\n",
    "    data_kde = np.linspace(-0.4, 0.4, 100)\n",
    "    data = fits.getdata(diffdatafile + '.fits').flatten()\n",
    "    data = data[~np.isnan(data)]\n",
    "    p_range = np.array([0.15865 * 100, 50., 0.84135 * 100]) # I believe this is one sigma? ask Jaime\n",
    "    vlsr_median_unc = np.round(np.percentile(data, p_range), decimals=2)\n",
    "\n",
    "    kernel = stats.gaussian_kde(data)\n",
    "    dens = kernel(data_kde)\n",
    "    dens /= simpson(dens, data_kde) # normalizing of probability\n",
    "\n",
    "    datasave = np.array([data_kde, dens, vlsr_median_unc], dtype=object)\n",
    "\n",
    "    np.save(diffdatafile+'.npy', datasave)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bd692c-49a2-4fe8-9875-e71e52d26745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure()\n",
    "# ax = fig.add_subplot(111)\n",
    "# ax.hist(data, fc='k', bins='fd', density=True)\n",
    "# ax.plot(data_kde, dens)\n",
    "# ax.set_xlim([-0.4, 0.4])\n",
    "# ax.plot([vlsr_median_unc[1], vlsr_median_unc[1]], [0.5, 2], color='C1', ls='--')\n",
    "# ax.text(vlsr_median_unc[1]-0.05,1.1,r'{0}$_{{{1:.2f}}}^{{+{2:.2f}}}$'.format(vlsr_median_unc[1], vlsr_median_unc[0]-vlsr_median_unc[1], vlsr_median_unc[2]-vlsr_median_unc[1]), size=10, color='C1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa96734b-44de-4b16-92f8-c803b704dbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This part only works for velocity\n",
    "# x is v_LSR of HC3N\n",
    "if not os.path.exists(diffdatafile+'_2DKDE.npy'):\n",
    "    xmin = 9.8\n",
    "    xmax = 10.8\n",
    "    # y is v_LSR of NH3\n",
    "    ymin = 9.8\n",
    "    ymax = 10.8\n",
    "    xx, yy = np.mgrid[xmin:xmax:100j, ymin:ymax:100j]\n",
    "    positions = np.vstack([xx.ravel(), yy.ravel()])\n",
    "    # we select only those who are not nan\n",
    "    gd_vlos = np.isfinite(HC3Nvlsrconvolved * newNH3image)\n",
    "    values = np.vstack([HC3Nvlsrconvolved[gd_vlos], newNH3image[gd_vlos]])\n",
    "    # we calculate the kernel distribution\n",
    "    kernel = stats.gaussian_kde(values)\n",
    "    zz = np.reshape(kernel(positions).T, xx.shape)\n",
    "    zz /= zz.max()\n",
    "\n",
    "    datasave = np.array([xx, yy, zz])\n",
    "\n",
    "    np.save(diffdatafile+'_2DKDE.npy', datasave)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f690769-3974-48b3-b1dc-75fdf71a55db",
   "metadata": {},
   "source": [
    "## Cuts along horizontal and vertical samples\n",
    "We cut a box around the protostar and determine the position of maximum difference to plot the velocities of both molecules in horizontal and vertical cuts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33516b6-509c-4c34-a37b-7cf3b45151dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_irs1 = SkyCoord(ra_yso*u.deg, dec_yso*u.deg, frame='fk5')\n",
    "boxsize = 24 # * u.arcsec\n",
    "boxcut = u.Quantity((boxsize, boxsize), u.arcsec)\n",
    "# boxsize = 50\n",
    "wcs_diff = WCS(newhead)\n",
    "cutout = Cutout2D(diff, position_irs1, boxcut, wcs=wcs_diff)\n",
    "wcs_diff_small = cutout.wcs\n",
    "diff_small = cutout.data\n",
    "boxsize_y, boxsize_x = np.shape(diff_small)\n",
    "y_min, x_min = np.unravel_index(np.nanargmin(diff_small), diff_small.shape)\n",
    "x_min_sky, y_min_sky = wcs_diff_small.all_pix2world(x_min, y_min, 0)\n",
    "x_min_sky, y_min_sky = float(x_min_sky), float(y_min_sky)\n",
    "if not os.path.exists(min_diff_HC3N_NH3_coords_file):\n",
    "    np.save(min_diff_HC3N_NH3_coords_file, np.array([x_min_sky, y_min_sky]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468c3af9-02a8-4193-babc-a0155e4a2e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to cut all images\n",
    "cutout_unc = Cutout2D(diff_unc, position_irs1, boxcut, wcs=wcs_diff)\n",
    "diff_unc_small = cutout_unc.data\n",
    "\n",
    "chansize_hc3n = 0.21 / np.sqrt(2*np.pi)/2.35\n",
    "chansize_nh3 = 0.04 / np.sqrt(2*np.pi)/2.35 # ask if they are independent\n",
    "cutout_hc3n = Cutout2D(HC3Nvlsrconvolved, position_irs1, boxcut, wcs=wcs_diff)\n",
    "hc3nvel_small = cutout_hc3n.data\n",
    "\n",
    "cutout_hc3n_u = Cutout2D(HC3Nvlsr_uconvolved, position_irs1, boxcut, wcs=wcs_diff)\n",
    "hc3nvel_small_u = cutout_hc3n_u.data\n",
    "\n",
    "cutout_nh3 = Cutout2D(newNH3image, position_irs1, boxcut, wcs=wcs_diff)\n",
    "nh3vel_small = cutout_nh3.data\n",
    "\n",
    "cutout_nh3_u = Cutout2D(newNH3uncimage, position_irs1, boxcut, wcs=wcs_diff)\n",
    "nh3vel_small_u = cutout_nh3_u.data\n",
    "\n",
    "xarray = (np.linspace(0, boxsize_x, boxsize_x) - x_min) * pixsizeHC3N * 3600 # arcsec\n",
    "yarray = (np.linspace(0, boxsize_y, boxsize_y) - y_min) * pixsizeHC3N * 3600 # arcsec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951d89f3-6b86-4497-9442-e9cd72aaa51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we get the mask of where the streamer is present\n",
    "streamer_vel, streamerhead = fits.getdata(filenameh2costreamer+'.fits', header=True)\n",
    "pixsizeH2CO = streamerhead['CDELT2']\n",
    "wcs_streamer = WCS(streamerhead)\n",
    "boxsize_y_stream, boxsize_x_stream = np.shape(streamer_vel)\n",
    "x_min_stream, y_min_stream = wcs_streamer.all_world2pix(x_min_sky, y_min_sky, 0)\n",
    "x_min_stream, y_min_stream = int(x_min_stream), int(y_min_stream)\n",
    "# TODO: Fix to get the correct position with respect to the difference map\n",
    "x_array_stream = (np.linspace(0, boxsize_x_stream, boxsize_x_stream) - x_min_stream) * pixsizeH2CO * 3600 # arcsec\n",
    "y_array_stream = (np.linspace(0, boxsize_y_stream, boxsize_y_stream) - y_min_stream) * pixsizeH2CO * 3600 # arcsec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a222d0-6a01-4d11-bc95-ce68e51b4ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "nh3vls_hor = nh3vel_small[y_min]\n",
    "nh3vls_hor_u = nh3vel_small_u[y_min]\n",
    "hc3nvls_hor = hc3nvel_small[y_min]\n",
    "hc3nvls_hor_u = hc3nvel_small_u[y_min]\n",
    "\n",
    "nh3vls_ver = nh3vel_small[:, x_min]\n",
    "nh3vls_ver_u = nh3vel_small_u[:, x_min]\n",
    "hc3nvls_ver = hc3nvel_small[:, x_min]\n",
    "hc3nvls_ver_u = hc3nvel_small_u[:, x_min]\n",
    "\n",
    "diff_hor = diff_small[y_min]\n",
    "diff_hor_u = diff_unc_small[y_min]\n",
    "diff_ver = diff_small[:, x_min]\n",
    "diff_ver_u = diff_unc_small[:, x_min]\n",
    "\n",
    "streamer_area_hor = ~np.isnan(streamer_vel[y_min_stream])\n",
    "streamer_area_ver = ~np.isnan(streamer_vel[:,x_min_stream])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d8ed6a-2162-4cb4-b4db-3fccac1f42b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here e save all the data to plot\n",
    "cuts_horizontal = np.array([xarray, nh3vls_hor, nh3vls_hor_u, hc3nvls_hor, hc3nvls_hor_u, diff_hor, diff_hor_u, x_array_stream, streamer_area_hor], dtype=object)\n",
    "cuts_vertical = np.array([yarray, nh3vls_ver, nh3vls_ver_u, hc3nvls_ver, hc3nvls_ver_u, diff_ver, diff_ver_u, y_array_stream, streamer_area_ver], dtype=object)\n",
    "\n",
    "if not os.path.exists(hor_cut_file): np.save(hor_cut_file, cuts_horizontal)\n",
    "if not os.path.exists(ver_cut_file): np.save(ver_cut_file, cuts_vertical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0520e098-0b44-4c35-8c3a-05847b142d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the uncertainty for NH3 vlsr is so small that the dominant value comes from HC3N\n",
    "fig = plt.figure(figsize=(6,4))\n",
    "ax = fig.add_subplot(121)\n",
    "ax.plot(xarray, diff_hor, color='k', marker='.')\n",
    "ax.fill_between(xarray, diff_hor-diff_hor_u, diff_hor+diff_hor_u, color='k', alpha=0.2)\n",
    "ax.set_title('Horizontal cut')\n",
    "ax.set_ylabel(r'$\\Delta V_{LSR}$ (km/s)')\n",
    "ax.fill_between(x_array_stream, 0,1, where=streamer_area_hor, alpha=0.5, transform=ax.get_xaxis_transform())\n",
    "ax.set_ylim([-0.1, 0.2])\n",
    "ax2 = fig.add_subplot(122)\n",
    "ax2.plot(yarray, diff_ver, color='k', marker='.')\n",
    "ax2.fill_between(yarray, diff_ver-diff_ver_u, diff_ver+diff_ver_u, color='k', alpha=0.2)\n",
    "ax2.set_title('Vertical cut')\n",
    "ax2.set_xlabel('Distance from minimum (arcsec)')\n",
    "ax2.fill_between(y_array_stream, 0,1, where=streamer_area_ver, alpha=0.5, transform=ax2.get_xaxis_transform())\n",
    "ax2.set_ylim([-0.1, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0cfe91-197f-4ea2-bbd6-c8ed11b36b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (4,4))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.errorbar(xarray, hc3nvls_hor, yerr=hc3nvls_hor_u, color='C0', marker='.', linestyle='None')\n",
    "ax.errorbar(xarray, nh3vls_hor, yerr=nh3vls_hor_u, color='C1', marker='.', linestyle='None')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73eccd23-3d60-47d9-b575-5bf3efebbc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (4,4))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.errorbar(yarray, hc3nvls_ver, yerr=hc3nvls_ver_u, color='C0', marker='o', linestyle='None')\n",
    "ax.errorbar(yarray, nh3vls_ver, yerr=chansize_nh3, color='C1', marker='o', linestyle='None')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
