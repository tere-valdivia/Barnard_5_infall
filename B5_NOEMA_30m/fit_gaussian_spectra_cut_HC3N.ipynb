{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "097fcb04",
   "metadata": {},
   "source": [
    "# Gaussian fit of HC$_3$N and H$_2$CO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f175619",
   "metadata": {},
   "source": [
    "## File and package preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22be256c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import all important packages\n",
    "import pyspeckit\n",
    "import numpy as np\n",
    "import astropy.units as u\n",
    "from astropy.wcs import WCS\n",
    "from astropy.io import fits\n",
    "from skimage.morphology import remove_small_objects, remove_small_holes, closing, disk, opening\n",
    "import os\n",
    "# import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import aplpy\n",
    "from B5setup import *\n",
    "from scipy.interpolate import griddata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef44fb65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Where we estimate the line is\n",
    "velinit = 9.0\n",
    "velend = 11.3\n",
    "starting_point = (107,166) #x, y\n",
    "snratio = 5\n",
    "velinitrms = 5.5\n",
    "multicore = 40\n",
    "minsizetrim = 100\n",
    "verbosity = False\n",
    "\n",
    "# File in K and in km/s\n",
    "fitdir = 'gaussfit/'\n",
    "imagefile = \"B5-NOEMA+30m-H2CO-1-01-0-00_cut_K\"  #\"B5-NOEMA+30m-H3CN-10-9_cut_K\"  # this comes from the CASA routine prepare_data.py\n",
    "rmsfile = fitdir + imagefile + '_rms'\n",
    "snrfile = fitdir + imagefile + '_snr'\n",
    "tpeakfile = fitdir + imagefile + '_Tpeak'\n",
    "maskfile = fitdir + imagefile + '_mask'\n",
    "momentfile = fitdir + imagefile + '_moments.fits'\n",
    "\n",
    "initguessfile = fitdir + imagefile + '_1G_guesses.fits'\n",
    "newguessfile = fitdir + imagefile + '_1G_interp_guesses.fits'\n",
    "\n",
    "fitfile =  fitdir + imagefile + '_1G_fitparams.fits'\n",
    "fitfilefiltered = fitdir + imagefile + '_1G_fitparams_filtered.fits'\n",
    "fitfile2 = fitdir + imagefile + '_1G_fitparams_2.fits'\n",
    "fitfile2filtered = fitdir + imagefile + '_1G_fitparams_2_filtered.fits'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a093e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the cube and additional images\n",
    "\n",
    "cube = pyspeckit.Cube(imagefile+'.fits')\n",
    "header = cube.header\n",
    "\n",
    "if not os.path.exists(rmsfile+'.fits'):\n",
    "    rmsmap = cube.slice(velinitrms, velinit, unit='km/s').cube.std(axis=0)\n",
    "    Tpeakmap = cube.slice(velinit, velend, unit='km/s').cube.max(axis=0)\n",
    "    snrmap = Tpeakmap / rmsmap\n",
    "    \n",
    "    hdcube=cube.header.copy()\n",
    "    key_remove=['NAXIS3','CRPIX3','CDELT3','CUNIT3','CTYPE3','CRVAL3','SPECSYS']\n",
    "    for key_i in key_remove:\n",
    "        hdcube.remove(key_i)\n",
    "    hdcube['WCSAXES'] = 2\n",
    "    hdcube['NAXIS'] = 2\n",
    "    fits.writeto(rmsfile+'.fits', rmsmap, hdcube)\n",
    "    print('Created RMS file')\n",
    "    fits.writeto(tpeakfile+'.fits', Tpeakmap, hdcube)\n",
    "    print('Created Tpeak file')\n",
    "    hdcubesnr = hdcube.copy()\n",
    "    hdcubesnr['BUNIT'] = ''\n",
    "    fits.writeto(snrfile+'.fits', snrmap, hdcubesnr)\n",
    "    print('Created SNR file')\n",
    "    \n",
    "\n",
    "else:\n",
    "    rmsmap = fits.getdata(rmsfile+'.fits')\n",
    "    Tpeakmap = fits.getdata(tpeakfile+'.fits')\n",
    "    snrmap = fits.getdata(snrfile+'.fits')\n",
    "    \n",
    "    \n",
    "if not os.path.exists(maskfile+'.fits'):\n",
    "    key_remove=['NAXIS3','CRPIX3','CDELT3','CUNIT3','CTYPE3','CRVAL3','SPECSYS']\n",
    "    hdcube=cube.header.copy()\n",
    "    for key_i in key_remove:\n",
    "        hdcube.remove(key_i)\n",
    "    hdcube['WCSAXES'] = 2\n",
    "    hdcube['NAXIS'] = 2\n",
    "    hdcube['BUNIT'] = ''\n",
    "    planemask = (snrmap > snratio)\n",
    "    fits.writeto(maskfile+'_0.fits', planemask.astype(int), hdcube)\n",
    "    # check the resulting mask map to see how much does the minimum size have to be and its connectivity\n",
    "    # before applying this filter\n",
    "    planemask = remove_small_objects(planemask, min_size=minsizetrim) # removes small islands of emission\n",
    "    fits.writeto(maskfile+'_1.fits', planemask.astype(int), hdcube)\n",
    "#     planemask = opening(planemask, disk(1))\n",
    "    planemask = remove_small_holes(planemask, area_threshold=minsizetrim) # fills small holes inside the emission area\n",
    "    fits.writeto(maskfile+'_2.fits', planemask.astype(int), hdcube)\n",
    "    planemask = closing(planemask, disk(6)) # fills cracks\n",
    "    fits.writeto(maskfile+'.fits', planemask.astype(int), hdcube)\n",
    "    print('Created Mask file')\n",
    "else:\n",
    "    planemask = fits.getdata(maskfile+'.fits')\n",
    "# initguesses = fits.getdata(initguessfile) #we have no initguesses for H2CO\n",
    "print('Loaded initial files, ready to fit')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89541f83",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Fitting\n",
    "\n",
    "We did a first fit with moments as initial guesses and now we use the result of that first pass as initial guess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754f2309-5622-4614-aa01-8b3c95825f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the moments as initial guesses because it can take forever\n",
    "if not os.path.exists(momentfile):\n",
    "    cube.momenteach(vheight=False)\n",
    "    fits.writeto(momentfile, cube.momentcube)\n",
    "else:\n",
    "    cube.momentcube = fits.getdata(momentfile)\n",
    "\n",
    "initguesses = cube.momentcube\n",
    "# initguesses = fits.getdata(initguessfile)[:3]\n",
    "# initguesses[0][np.where(initguesses[0]<0)] = 0\n",
    "# initguesses[1][np.where(initguesses[1]>velend)] = velend\n",
    "# initguesses[1][np.where(initguesses[1]<velinit)] = velinit\n",
    "# initguesses[2][np.where(initguesses[2]>velend-velinit)] = velend-velinit\n",
    "# initguesses[0][np.where(np.isnan(initguesses[0]))] = 1\n",
    "# initguesses[1][np.where(np.isnan(initguesses[1]))] = 10.2\n",
    "# initguesses[2][np.where(np.isnan(initguesses[2]))] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57741d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(fitfile):\n",
    "    cube.fiteach(fittype='gaussian',\n",
    "                 signal_cut=snratio,\n",
    "                 guesses=initguesses,\n",
    "                 errmap = rmsmap, \n",
    "                 maskmap = planemask,\n",
    "                 limitedmin=[True, True, True],\n",
    "                 limitedmax=[False, True, True],\n",
    "                 minpars=[0, velinit, 0],\n",
    "                 maxpars=[0, velend, velend-velinit],\n",
    "                 use_neighbor_as_guess=True, \n",
    "                 start_from_point=starting_point,\n",
    "                 verbose=verbosity,\n",
    "                 multicore=multicore)\n",
    "    cube.write_fit(fitfile)\n",
    "    fittedmodel = cube.get_modelcube()\n",
    "    \n",
    "else:\n",
    "    cube.load_model_fit(fitfile, 3, fittype='gaussian')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0c9490",
   "metadata": {},
   "source": [
    "### Quality asessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75238617",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filtersolutions(spc, rmsmap, snratio, velinit, velend, filter_negative=False, errorfrac=10, eps=1.e-6):\n",
    "    \"\"\"\n",
    "    Replace the pixels in the fitted cube with np.nan where the fit is not\n",
    "    good enough according to our criteria.\n",
    "\n",
    "    The criteria that a pixel must have are:\n",
    "    - The errors are not zero (less than eps)\n",
    "    - The peak must not be negative in case filter_negatives is true\n",
    "    - The error fraction is lower than errorfrac, not applied if errorfrac>=1\n",
    "    - The moment 1 value must be within the range [velinit,velend]\n",
    "    - The peak value must be larger than rms times rmslevel\n",
    "    - The weighted velocity dispersion must be smaller than the absolute\n",
    "    value of velend-velinit\n",
    "    - If one parameter in a spectra is np.nan, all the spectra must be nan (sanity\n",
    "    check)\n",
    "    - All points must be within a region (part of the input)\n",
    "\n",
    "    Args:\n",
    "        variable (type): description\n",
    "\n",
    "    Returns:\n",
    "        type: description\n",
    "\n",
    "    Raises:\n",
    "        Exception: description\n",
    "\n",
    "    \"\"\"\n",
    "    # we first create all the masks we need\n",
    "    \n",
    "    # all errors must be non zero\n",
    "    # note that eps must be larger than the velocity dispersion\n",
    "    zeromask = np.where(np.abs(spc.errcube[0])<eps, 1, 0)\\\n",
    "    + np.where(np.abs(spc.errcube[1])<eps, 1, 0)\\\n",
    "    + np.where(np.abs(spc.errcube[2])<eps, 1, 0)\n",
    "    \n",
    "    if errorfrac < 1.0:\n",
    "        errormask = np.where(np.abs(spc.errcube[0]/spc.parcube[0]) > errorfrac, 1, 0)\\\n",
    "            + np.where(np.abs(spc.errcube[1]/spc.parcube[1]) > errorfrac, 1, 0)\\\n",
    "            + np.where(np.abs(spc.errcube[2]/spc.parcube[2]) > errorfrac, 1, 0)\n",
    "    else:\n",
    "        errormask = np.zeros(np.shape(zeromask), dtype=int)\n",
    "        \n",
    "    if filter_negative:\n",
    "        negativemask = np.where(spc.parcube[0] < 0, 1, 0)\\\n",
    "        + np.where(spc.parcube[1] < 0, 1, 0) \\\n",
    "        + np.where(spc.parcube[2] < 0, 1, 0)\n",
    "        \n",
    "    else:\n",
    "        negativemask = np.zeros(np.shape(zeromask), dtype=int)\n",
    "    \n",
    "    velocitymask = np.where(spc.parcube[1] < velinit, 1, 0) + \\\n",
    "        np.where(spc.parcube[1] > velend, 1, 0)\n",
    "    snrmappeak = spc.parcube[0] / rmsmap\n",
    "    peakmask = np.where(snrmappeak < snratio, 1, 0)\n",
    "    \n",
    "    nanmask = np.where(np.isnan(spc.parcube[0]), 1, 0)\\\n",
    "        + np.where(np.isnan(spc.parcube[1]), 1, 0)\\\n",
    "        + np.where(np.isnan(spc.parcube[2]), 1, 0)\\\n",
    "        + np.where(np.isnan(spc.errcube[0]), 1, 0)\\\n",
    "        + np.where(np.isnan(spc.errcube[1]), 1, 0)\\\n",
    "        + np.where(np.isnan(spc.errcube[2]), 1, 0)\n",
    "    \n",
    "    finalmask = zeromask + errormask + negativemask + velocitymask + peakmask + nanmask\n",
    "        \n",
    "    spc.parcube[np.where(np.repeat([finalmask], 3, axis=0))] = np.nan\n",
    "    spc.errcube[np.where(np.repeat([finalmask], 3, axis=0))] = np.nan\n",
    "    \n",
    "    # eliminate isolated small islands of emission after the filter\n",
    "    \n",
    "    planemask = ~np.isnan(spc.parcube[0])\n",
    "    planemask = remove_small_objects(planemask, min_size=minsizetrim)\n",
    "    smallmask = np.ones(np.shape(planemask), dtype=int) - planemask\n",
    "    spc.parcube[np.where(np.repeat([smallmask], 3, axis=0))] = np.nan\n",
    "    spc.errcube[np.where(np.repeat([smallmask], 3, axis=0))] = np.nan\n",
    "    return spc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd93731",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we apply the filter\n",
    "if not os.path.exists(fitfilefiltered):\n",
    "    cube = filtersolutions(cube, rmsmap, snratio, velinit, velend, filter_negative=True, errorfrac=0.5)\n",
    "    cube.write_fit(fitfilefiltered)\n",
    "    fittedmodel = cube.get_modelcube()\n",
    "        \n",
    "else:\n",
    "    cube.load_model_fit(fitfilefiltered, 3, fittype='gaussian')\n",
    "    fittedmodel = cube.get_modelcube()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a45ef69-c105-43aa-b1cf-ce1aab7399a5",
   "metadata": {},
   "source": [
    "### Second fit: Interpolation of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58157c4-f526-42f8-ba42-6fa51d61403e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolatesolutions(solfilein, npeaks, mask=None):\n",
    "    '''\n",
    "    The solfilein must be a .fits file that contains one parameter per \n",
    "    plane and then one parameter uncertainty per plane.\n",
    "    The shape must be [nplane, yy, xx]\n",
    "    The mask must be 2 dimensional\n",
    "    '''\n",
    "    solcube = fits.getdata(solfilein)[:3*npeaks]\n",
    "    if np.any(np.isnan(solcube)):\n",
    "        solcube[np.where(np.isnan(solcube))] = 0\n",
    "    solcubeshape = np.shape(solcube)\n",
    "    yy, xx = np.indices(solcubeshape[1:])\n",
    "    filledcube = solcube.copy()\n",
    "    headersolcube = fits.getheader(solfilein)\n",
    "\n",
    "    for i, plane in enumerate(solcube):\n",
    "        indexknown = np.where(plane<1e-5, False, True)\n",
    "        filledcube[i][~indexknown] = griddata((xx[indexknown], yy[indexknown]),\n",
    "                                                  plane[indexknown],\n",
    "                                                  (xx[~indexknown], yy[~indexknown])\n",
    "                                                 )\n",
    "        if mask is not None:\n",
    "            filledcube[i][np.where(mask==0)] = np.nan\n",
    "    return filledcube, headersolcube\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed4ed00-fbd5-43d1-8c33-c2b04c2557cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(newguessfile):\n",
    "    print(\"Interpolating previous solutions.\")\n",
    "    newinitguess, headerguess = interpolatesolutions(fitfilefiltered, 1, mask=planemask)\n",
    "    fits.writeto(newguessfile, newinitguess, headerguess)\n",
    "    \n",
    "else:\n",
    "    print(\"Interpolation exists. Loading.\")\n",
    "    newinitguess = fits.getdata(newguessfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1260bc97-fadb-4b0b-a1a5-d786b0970fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(fitfile2):\n",
    "    print(\"Starting fit (S/N>3)\")\n",
    "    cube.fiteach(fittype='gaussian',\n",
    "                 guesses=newinitguess,\n",
    "                 errmap = rmsmap, \n",
    "                 maskmap = planemask,\n",
    "                 limitedmin=[True, True, True],\n",
    "                 limitedmax=[False, True, True],\n",
    "                 minpars=[0, velinit, 0],\n",
    "                 maxpars=[0, velend, velend-velinit],\n",
    "                 use_neighbor_as_guess=True, \n",
    "                 start_from_point=starting_point,\n",
    "                 verbose=verbosity,\n",
    "                 multicore=multicore)\n",
    "    cube.write_fit(fitfile2)\n",
    "    fittedmodel = cube.get_modelcube()\n",
    "    \n",
    "else:\n",
    "    print(\"Fit 2 exists. Loading\")\n",
    "    cube.load_model_fit(fitfile2, 3, fittype='gaussian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0172ba00-187e-4468-90fd-a77f17679e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we apply the filter but this time dropping the snratio to 3\n",
    "if not os.path.exists(fitfile2filtered):\n",
    "    cube = filtersolutions(cube, rmsmap, 3, velinit, velend, filter_negative=True, errorfrac=0.5)\n",
    "    cube.write_fit(fitfile2filtered)\n",
    "    fittedmodel = cube.get_modelcube()\n",
    "        \n",
    "else:\n",
    "    cube.load_model_fit(fitfile2filtered, 3, fittype='gaussian')\n",
    "    fittedmodel = cube.get_modelcube()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564a2ce6",
   "metadata": {},
   "source": [
    "There is a significant improvement from the first fit to the second in the southern tail, which is what we were looking for. Maybe it is worth to do a third pass, but for now, success!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3899fe0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Plotting the results \n",
    "\n",
    "This is just to observe them quickly. The plots will be more polished in figures_B5.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ea8736",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(1, figsize=(16,8))\n",
    "figname = fitdir + imagefile + '_1G_fitparams_filtered_results.pdf'\n",
    "\n",
    "paramscube = cube.parcube\n",
    "errcube = cube.errcube\n",
    "headerplot = fits.getheader(tpeakfile + '.fits')\n",
    "stretch = 'linear'\n",
    "vmin = [0, np.round(np.nanmin(paramscube[1]), 1), 0]\n",
    "vmax = [np.round(np.nanmax(paramscube[0]), 1), np.round(np.nanmax(paramscube[1]), 1), np.round(np.nanmax(paramscube[2]), 1)]\n",
    "cmap = ['viridis', 'RdYlBu_r', 'inferno']\n",
    "quantities = [r'$T_{\\mathrm{peak}}$', r'$V_{LSR}$', r'$\\sigma_{\\mathrm{r}}$']\n",
    "\n",
    "for i in range(len(paramscube)):\n",
    "    ax0 = plot_aplpy_subfig(fits.PrimaryHDU(paramscube[i], headerplot), fig, (1,3,i+1), stretch, \\\n",
    "                            vmin[i], vmax[i], cmap[i])\n",
    "    ax0.colorbar.set_axis_label_text(quantities[i])\n",
    "    if i:\n",
    "        ax0.tick_labels.hide()\n",
    "        ax0.axis_labels.hide()\n",
    "        \n",
    "fig.savefig(figname, bbox_inches='tight', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8658951a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Separating the components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3024ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdparams=cube.header.copy()\n",
    "key_remove=['NAXIS3','CRPIX3','CDELT3','CUNIT3','CTYPE3','CRVAL3','SPECSYS']\n",
    "for key_i in key_remove:\n",
    "    hdparams.remove(key_i)\n",
    "hdparams['WCSAXES'] = 2\n",
    "hdparams['NAXIS'] = 2\n",
    "hdparams['BUNIT'] = ''\n",
    "parameters = ['Tpeak', 'Vlsr', 'SigmaV']\n",
    "for i in range(len(paramscube)):\n",
    "    if not os.path.exists(fitfile2filtered[:len(fitfile2filtered)-5]+'_'+parameters[i]+'.fits'):\n",
    "        fits.writeto(fitfile2filtered[:len(fitfile2filtered)-5]+'_'+parameters[i]+'.fits', paramscube[i], hdparams)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0fae7d",
   "metadata": {},
   "source": [
    "We can also plot with respect to the velocity for B5-IRS1 found in Pineda+15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf24edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "vlsr_irs1 = 10.2 #km/s\n",
    "diffvlsr = paramscube[1] - vlsr_irs1\n",
    "diffmin = -0.5\n",
    "diffmax = 0.5\n",
    "headerplot = fits.getheader(fitdir + imagefile + '_1G_fitparams_filtered_Vlsr.fits')\n",
    "fig = plt.figure(1, figsize=(4,4))\n",
    "figname = fitdir + imagefile + '_1G_fitparams_filtered_Vlsr_minus_irs1.pdf'\n",
    "\n",
    "ax = plot_aplpy_subfig(fits.PrimaryHDU(diffvlsr, headerplot), \n",
    "                       fig, (1,1,1), 'linear', diffmin, \n",
    "                       diffmax, 'RdYlBu_r')\n",
    "ax.add_label(0.6, 0.04, \n",
    "             r'$V_{\\mathrm{LSR}} - V_{\\mathrm{LSR}, \\mathrm{B5-IRS1}}$', \n",
    "             relative=True, family='sans-serif', size=12)\n",
    "ax.show_markers(ra_yso, dec_yso, s=50, marker='*', edgecolor='k', facecolor='None')\n",
    "ax.show_contour('../B5_wide_multiple/data/B5_VLA_GBT_model_11_mom0.fits', colors='k', linewidths=0.5, levels=[0.1], zorder=34 )\n",
    "# fig.savefig(figname, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f4d506",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Two Gaussian fit towards B5-IRS1 for HC$_3$N\n",
    "\n",
    "The spectra around IRS1 protostar (Per-emb-53) shows that it might be better fitted with two Gaussians instead of one. We create a subcube with only the vicinity of IRS1 and fit 2 Gaussians there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f7b796",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spectral_cube import SpectralCube # we need this to cut\n",
    "x0, y0, x1, y1 = 83, 131, 133, 180"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fae6f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the subcube\n",
    "if not os.path.exists(imagefile+'_B5zoom.fits'):\n",
    "    cube_SC = SpectralCube.read(imagefile+'.fits')\n",
    "    subcube = cube_SC[:, y0:y1, x0:x1]\n",
    "    # we save the new cube with its wcs information\n",
    "    subcube.write(imagefile+'_B5zoom.fits')\n",
    "    \n",
    "subcube = pyspeckit.Cube(imagefile+'_B5zoom.fits')\n",
    "subheader = subcube.header\n",
    "subinitguesses = [2.1, 10.2, 0.2, 0.6, 9.2, 0.2] # we just use the same guesses for all\n",
    "submask = fits.getdata(maskfile+'.fits')[y0:y1, x0:x1]\n",
    "subrmsmap = fits.getdata(rmsfile+'.fits')[y0:y1, x0:x1]\n",
    "subfitfile = fitdir + imagefile + '_B5zoom_2G_fitparams.fits'\n",
    "subfitfilefiltered = fitdir + imagefile + '_B5zoom_2G_fitparams_filtered.fits'\n",
    "substarting_point = (25, 24)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01febedc",
   "metadata": {},
   "source": [
    "## Fitting\n",
    "This is the first fit where we use fixed initial guesses for all pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6ddd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(subfitfile):\n",
    "    subcube.fiteach(fittype='gaussian',\n",
    "                 guesses=subinitguesses,\n",
    "                 signal_cut=snratio,\n",
    "                 errmap = subrmsmap, \n",
    "                 maskmap = submask,\n",
    "                 limitedmin=[True, True, True, True, True, True],\n",
    "                 limitedmax=[False, True, True, False, True, True],\n",
    "                 minpars=[0, velinit, 0, 0, velinit, 0],\n",
    "                 maxpars=[0, velend, 0.5, 0, velend, 0.5],\n",
    "                 use_neighbor_as_guess=True, \n",
    "                 start_from_point=substarting_point,\n",
    "                 verbose_level=1,\n",
    "                 multicore=multicore)\n",
    "    subcube.write_fit(subfitfile)\n",
    "    \n",
    "else:\n",
    "    subcube.load_model_fit(subfitfile, 3, npeaks=2, fittype='gaussian')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f59de7c",
   "metadata": {},
   "source": [
    "We checked manually if the fit is ok with check_fit_HC3N_interactive.py.\n",
    "\n",
    "Now we use the obtained values as initial guesses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810e6f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting round 2\n",
    "subinitguesses2 = subcube.parcube\n",
    "subfitfile2 = fitdir + imagefile + '_B5zoom_2G_fitparams2.fits'\n",
    "subfitfilefiltered2 = fitdir + imagefile + '_B5zoom_2G_fitparams2_filtered.fits'\n",
    "\n",
    "if not os.path.exists(subfitfile2):\n",
    "    subcube.fiteach(fittype='gaussian',\n",
    "                 guesses=subinitguesses2,\n",
    "                 signal_cut=snratio,\n",
    "                 errmap = subrmsmap, \n",
    "                 maskmap = submask,\n",
    "                 limitedmin=[True, True, True, True, True, True],\n",
    "                 limitedmax=[False, True, True, False, True, True],\n",
    "                 minpars=[0, velinit, 0, 0, velinit, 0],\n",
    "                 maxpars=[0, velend, 0.5, 0, velend, 0.5],\n",
    "                 use_neighbor_as_guess=True, \n",
    "                 start_from_point=substarting_point,\n",
    "                 verbose_level=1,\n",
    "                 multicore=multicore)\n",
    "    subcube.write_fit(subfitfile2)\n",
    "    \n",
    "else:\n",
    "    subcube.load_model_fit(subfitfile2, 3, npeaks=2, fittype='gaussian')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4fe248",
   "metadata": {},
   "source": [
    "We checked the second run with the same code as before."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac17591f",
   "metadata": {},
   "source": [
    "#### Quality asessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a0c5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "minsizetrim2 = 5\n",
    "def filtersolutions2(spc, rmsmap, snratio, velinit, velend, filter_negative=False, checkseparation=False, maxwidth=2e9, errorfrac=10, eps=1.e-6):\n",
    "    \"\"\"\n",
    "    Replace the pixels in the fitted cube with np.nan where the fit is not\n",
    "    good enough according to our criteria.\n",
    "\n",
    "    The criteria that a pixel must have are:\n",
    "    - The errors are not zero (less than eps)\n",
    "    - The peak must not be negative in case filter_negatives is true\n",
    "    - The error fraction is lower than errorfrac, not applied if errorfrac>=1\n",
    "    - The moment 1 value must be within the range [velinit,velend]\n",
    "    - The peak value must be larger than rms times rmslevel\n",
    "    - The weighted velocity dispersion must be smaller than the absolute\n",
    "    value of velend-velinit\n",
    "    - If one parameter in a spectra is np.nan, all the spectra must be nan (sanity\n",
    "    check)\n",
    "    - All points must be within a region (part of the input)\n",
    "\n",
    "    Args:\n",
    "        variable (type): description\n",
    "\n",
    "    Returns:\n",
    "        type: description\n",
    "\n",
    "    Raises:\n",
    "        Exception: description\n",
    "\n",
    "    \"\"\"\n",
    "    # we first create all the masks we need\n",
    "    \n",
    "    # all errors must be non zero\n",
    "    # note that eps must be larger than the velocity dispersion\n",
    "    zeromask = np.where(np.abs(spc.errcube[0])<eps, 1, 0)\\\n",
    "    + np.where(np.abs(spc.errcube[1])<eps, 1, 0)\\\n",
    "    + np.where(np.abs(spc.errcube[2])<eps, 1, 0)\\\n",
    "    + np.where(np.abs(spc.errcube[3])<eps, 1, 0)\\\n",
    "    + np.where(np.abs(spc.errcube[4])<eps, 1, 0)\\\n",
    "    + np.where(np.abs(spc.errcube[5])<eps, 1, 0)\n",
    "    \n",
    "    if errorfrac < 10:\n",
    "        errormask = np.where(np.abs(spc.errcube[0]/spc.parcube[0]) > errorfrac, 1, 0)\\\n",
    "            + np.where(np.abs(spc.errcube[1]/spc.parcube[1]) > errorfrac, 1, 0)\\\n",
    "            + np.where(np.abs(spc.errcube[2]/spc.parcube[2]) > errorfrac, 1, 0)\\\n",
    "            + np.where(np.abs(spc.errcube[3]/spc.parcube[3]) > errorfrac, 1, 0)\\\n",
    "            + np.where(np.abs(spc.errcube[4]/spc.parcube[4]) > errorfrac, 1, 0)\\\n",
    "            + np.where(np.abs(spc.errcube[5]/spc.parcube[5]) > errorfrac, 1, 0)\n",
    "    else:\n",
    "        errormask = np.zeros(np.shape(zeromask), dtype=int)\n",
    "        \n",
    "    if filter_negative:\n",
    "        negativemask = np.where(spc.parcube[0] < 0, 1, 0)\\\n",
    "        + np.where(spc.parcube[1] < 0, 1, 0) \\\n",
    "        + np.where(spc.parcube[2] < 0, 1, 0) \\\n",
    "        + np.where(spc.parcube[3] < 0, 1, 0) \\\n",
    "        + np.where(spc.parcube[4] < 0, 1, 0) \\\n",
    "        + np.where(spc.parcube[5] < 0, 1, 0)\n",
    "        \n",
    "    else:\n",
    "        negativemask = np.zeros(np.shape(zeromask), dtype=int)\n",
    "    \n",
    "    velocitymask = np.where(spc.parcube[1] < velinit, 1, 0) \\\n",
    "                    + np.where(spc.parcube[1] > velend, 1, 0)\\\n",
    "                    + np.where(spc.parcube[4] > velend, 1, 0)\\\n",
    "                    + np.where(spc.parcube[4] < velinit, 1, 0)\n",
    "    \n",
    "    snrmappeak = spc.parcube[0] / rmsmap\n",
    "    peakmask = np.where(snrmappeak < snratio, 1, 0)\n",
    "    snrmappeak2 = spc.parcube[3] / rmsmap\n",
    "    peakmask2 = np.where(snrmappeak2 < snratio, 1, 0)\n",
    "    \n",
    "    nanmask = np.where(np.isnan(spc.parcube[0]), 1, 0)\\\n",
    "        + np.where(np.isnan(spc.parcube[1]), 1, 0)\\\n",
    "        + np.where(np.isnan(spc.parcube[2]), 1, 0)\\\n",
    "        + np.where(np.isnan(spc.parcube[3]), 1, 0)\\\n",
    "        + np.where(np.isnan(spc.parcube[4]), 1, 0)\\\n",
    "        + np.where(np.isnan(spc.parcube[5]), 1, 0)\\\n",
    "        + np.where(np.isnan(spc.errcube[0]), 1, 0)\\\n",
    "        + np.where(np.isnan(spc.errcube[1]), 1, 0)\\\n",
    "        + np.where(np.isnan(spc.errcube[2]), 1, 0)\\\n",
    "        + np.where(np.isnan(spc.errcube[3]), 1, 0)\\\n",
    "        + np.where(np.isnan(spc.errcube[4]), 1, 0)\\\n",
    "        + np.where(np.isnan(spc.errcube[5]), 1, 0)\\\n",
    "    \n",
    "    if checkseparation:\n",
    "        veldifmask = np.where(np.abs(spc.parcube[1]-spc.parcube[4])>spc.parcube[2]*3, 1, 0)\n",
    "    else:\n",
    "        veldifmask = np.zeros(np.shape(zeromask), dtype=int)\n",
    "        \n",
    "    if maxwidth<1e9:\n",
    "        widthmask = np.where(spc.parcube[5]>maxwidth, 1, 0)\n",
    "    else:\n",
    "        widthmask = np.zeros(np.shape(zeromask), dtype=int)\n",
    "    finalmask = zeromask + errormask + negativemask  + nanmask + velocitymask + veldifmask + widthmask #+ peakmask + peakmask2 \n",
    "        \n",
    "    spc.parcube[np.where(np.repeat([finalmask], 6, axis=0))] = np.nan\n",
    "    spc.errcube[np.where(np.repeat([finalmask], 6, axis=0))] = np.nan\n",
    "    \n",
    "    # eliminate isolated small islands of emission after the filter\n",
    "    \n",
    "    planemask = ~np.isnan(spc.parcube[0])\n",
    "    planemask = remove_small_objects(planemask, min_size=minsizetrim2)\n",
    "    smallmask = np.ones(np.shape(planemask), dtype=int) - planemask\n",
    "    spc.parcube[np.where(np.repeat([smallmask], 6, axis=0))] = np.nan\n",
    "    spc.errcube[np.where(np.repeat([smallmask], 6, axis=0))] = np.nan\n",
    "    return spc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c43ba3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we apply the filter\n",
    "velinit2 = 8.9 #we are a bit more relaxed with the limits this time\n",
    "velend2 = 10.3\n",
    "if not os.path.exists(subfitfilefiltered2):\n",
    "    subcube = filtersolutions2(subcube, subrmsmap, 1.5, velinit2, velend2, filter_negative=True, checkseparation=True, maxwidth=0.25, errorfrac=3)\n",
    "    subcube.write_fit(subfitfilefiltered2)\n",
    "    fittedmodel2 = subcube.get_modelcube()\n",
    "        \n",
    "else:\n",
    "    subcube.load_model_fit(subfitfilefiltered2, 3, npeaks=2, fittype='gaussian')\n",
    "    fittedmodel2 = subcube.get_modelcube()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9feb78",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Decomposition\n",
    "\n",
    "Here we take the 1 and 2 Gaussian fits and see what physical components are present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21240d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we evaluate if we recover more information with the two Gaussian fit or not\n",
    "def chi_square(yPred, yData, err):\n",
    "    chi2 = np.sum((yPred-yData)**2/(err**2))\n",
    "    return chi2\n",
    "\n",
    "def chi_square_reduced(yPred, yData, err, k):\n",
    "    chi2 = chi_square(yPred, yData, err)\n",
    "    chi2red = chi2 / (len(yData)-k)\n",
    "    return chi2red\n",
    "\n",
    "def AIC(yPred, data, err, k):\n",
    "    \"\"\"\n",
    "    Returns the Akaike information criterion (AIC) for a given function with a\n",
    "    number of parameters k and a negative log-likelihood value given\n",
    "    by func(data, params)\n",
    "    \"\"\"\n",
    "    # ll = log_likelihood(yPred, data, err)\n",
    "    # aic = 2 * k + 2 * ll\n",
    "    chi2 = chi_square(yPred, data, err)\n",
    "    aic = 2 * k + chi2 # we leave out the constant because it is the same for\n",
    "    # both models\n",
    "    return aic\n",
    "\n",
    "def probaic(aicmin, aiclist):\n",
    "    return np.exp((aicmin-aiclist)/2)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66d8e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fittedmodel1 = cube.get_modelcube()[:, y0:y1, x0:x1]\n",
    "print(np.shape(fittedmodel1), np.shape(fittedmodel2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece8cda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.modeling.functional_models import Gaussian1D\n",
    "from astropy.wcs import WCS\n",
    "# we define the variables we need\n",
    "aic1map = np.zeros(np.shape(subinitguesses2[0])) * np.nan\n",
    "aic2map = np.zeros(np.shape(subinitguesses2[0])) * np.nan\n",
    "\n",
    "aic1mapfile = subfitfile[:len(subfitfile)-5] + '_aicmap1g.fits'\n",
    "aic2mapfile = subfitfile[:len(subfitfile)-5] + '_aicmap2g.fits'\n",
    "\n",
    "params1g = cube.parcube[:, y0:y1, x0:x1]\n",
    "print(np.shape(params1g))\n",
    "params2g = subcube.parcube\n",
    "print(np.shape(params2g))\n",
    "# This mask is 1 where the best fit is done with 2 Gaussians\n",
    "mask2g = np.zeros(np.shape(subinitguesses2[0]), dtype=int)\n",
    "mask2gfile = subfitfile[:len(subfitfile)-5] + '_2gmask.fits'\n",
    "probtolerance = 0.05 # 95% or over probability that it actually does recover information better\n",
    "flag_prob = np.zeros(np.shape(subinitguesses2[0]), dtype=int)\n",
    "flagprobfile = subfitfile[:len(subfitfile)-5] + '_2gflagprob.fits'\n",
    "problist = np.zeros(np.shape(subinitguesses2[0]), dtype=int) * np.nan\n",
    "probfile = subfitfile[:len(subfitfile)-5] + '_2prob.fits'\n",
    "\n",
    "chi1mapfile = subfitfile[:len(subfitfile)-5] + '_chi1g.fits'\n",
    "chi2mapfile = subfitfile[:len(subfitfile)-5] + '_chi2g.fits'\n",
    "chi1map = np.zeros(np.shape(subinitguesses2[0]))\n",
    "chi2map = np.zeros(np.shape(subinitguesses2[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8135cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we run the AIC test\n",
    "for y in range(y1-y0):\n",
    "    for x in range(x1-x0):\n",
    "        velocity = subcube.get_spectrum(x, y).xarr.value\n",
    "        spectrum = subcube.get_spectrum(x, y).data\n",
    "        param1G = params1g[:, y, x]\n",
    "        param2G = params2g[:, y, x]\n",
    "        if np.all(np.isnan(param1G)) or np.all(np.isnan(param2G)):\n",
    "            # If one of the fits failed, the comparison does not make sense\n",
    "            continue\n",
    "        ypred1g = np.flip(fittedmodel1[:, y, x])\n",
    "        ypred2g = np.flip(fittedmodel2[:, y, x])\n",
    "        aic1g = AIC(ypred1g, spectrum, subrmsmap[y, x], 3)\n",
    "        aic1map[y,x] = aic1g\n",
    "        aic2g = AIC(ypred2g, spectrum, subrmsmap[y, x], 6)\n",
    "        aic2map[y,x] = aic2g\n",
    "        chi1 = chi_square_reduced(ypred1g, spectrum, subrmsmap[y, x], 3)\n",
    "        chi2 = chi_square_reduced(ypred2g, spectrum, subrmsmap[y, x], 6)\n",
    "        chi1map[y, x] = chi1\n",
    "        chi2map[y, x] = chi2\n",
    "        # choose the minimum AIC\n",
    "        if aic2g < aic1g: #that 2G are best\n",
    "            mask2g[y, x] = 1\n",
    "            # we evaluate the probability that the other model is as good for\n",
    "            # minimizing information loss as the best one\n",
    "            prob = np.exp((aic2g - aic1g)/2.)\n",
    "            problist[y, x] = prob\n",
    "            if prob > probtolerance:\n",
    "                flag_prob[y,x] = 1\n",
    "        else:\n",
    "            prob = np.exp((aic1g - aic2g)/2.)\n",
    "            problist[y, x] = prob\n",
    "            if prob > probtolerance:\n",
    "                flag_prob[y,x] = 1\n",
    "            \n",
    "aicheader = WCS(subcube.header).celestial.to_header()\n",
    "if not os.path.exists(aic1mapfile): fits.writeto(aic1mapfile, aic1map, aicheader)\n",
    "if not os.path.exists(aic2mapfile): fits.writeto(aic2mapfile, aic2map, aicheader)\n",
    "if not os.path.exists(mask2gfile): fits.writeto(mask2gfile, mask2g, aicheader)\n",
    "if not os.path.exists(flagprobfile): fits.writeto(flagprobfile, flag_prob, aicheader)\n",
    "if not os.path.exists(probfile): fits.writeto(probfile, problist, aicheader)\n",
    "if not os.path.exists(chi1mapfile): fits.writeto(chi1mapfile, chi1map, aicheader)\n",
    "if not os.path.exists(chi2mapfile): fits.writeto(chi2mapfile, chi2map, aicheader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f70a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a test spectra to see if we are calculating the AIC and arranging the\n",
    "# spectra correctly in the loop\n",
    "xtest, ytest = 22,20\n",
    "velocity = subcube.get_spectrum(xtest, ytest).xarr.value\n",
    "spectrum = subcube.get_spectrum(xtest, ytest).data\n",
    "ypred1g = np.flip(fittedmodel1[:, ytest,xtest])\n",
    "ypred2g = np.flip(fittedmodel2[:, ytest,xtest])\n",
    "rms = subrmsmap[ytest,xtest]\n",
    "chi1g = chi_square(ypred1g, spectrum, rms)\n",
    "chi2g = chi_square(ypred2g, spectrum, rms)\n",
    "aic1g = AIC(ypred1g, spectrum, rms, 3)\n",
    "aic2g = AIC(ypred2g, spectrum, rms, 6)\n",
    "print(chi1g, chi2g)\n",
    "plt.plot(velocity, spectrum, 'k', drawstyle='steps-mid')\n",
    "plt.plot(velocity, ypred1g, 'r')\n",
    "plt.plot(velocity, ypred2g, 'g')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8e1a33",
   "metadata": {},
   "source": [
    "What we see in the maps is the following:\n",
    "- There is a small zone towards the east of the blob where, according to the AIC criterion, the 1 Gaussian fits better than the 2 Gaussian fit\n",
    "- The probability of the 2 Gaussian fit to recover as much information (or more) as the 1 Gaussian fit in the zone described above is much higher than 5% (its between 30 and 90% usually)\n",
    "- Towards the west of the blob, 2 Gaussians fit better than 1 Gaussian according to the AIC criterion. The probability of 1 Gaussian recovering the same information there is less than 5%\n",
    "\n",
    "A visual inspection of the fitted curves in the spectra convinces me that the 2 Gaussian fit is best, so in all the blob we will replace the previous 1 Gaussian fit with the strongest component of the 2 Gaussian fit. \n",
    "\n",
    "We will call this new map the hook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdc3555",
   "metadata": {},
   "source": [
    "### Creation of the hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949f8865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we take the cut map and replace the pixels which have a 2 Gaussian fit with the strongest\n",
    "# component\n",
    "index_2G = np.where(~np.isnan(params2g[0, :, :]))\n",
    "params1g = cube.parcube[:, y0:y1, x0:x1]\n",
    "errors1g = cube.errcube[:, y0:y1, x0:x1]\n",
    "for y, x in zip(index_2G[0], index_2G[1]):\n",
    "    params1g[:,y,x] = params2g[:3, y, x]\n",
    "    errors1g[:,y,x] = subcube.errcube[:3, y, x]\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8eb802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we plot it quickly to see it and save the small cube\n",
    "smallcubehookparamfile = subfitfile[:len(subfitfile)-5] + '_hookonlyparams.fits'\n",
    "smallcubehookparams = np.concatenate([params1g, errors1g])\n",
    "header = fits.getheader(subfitfilefiltered2)\n",
    "header['NAXIS'] = 6\n",
    "if not os.path.exists(smallcubehookparamfile):\n",
    "    fits.writeto(smallcubehookparamfile, smallcubehookparams, header)\n",
    "\n",
    "fig = plt.figure(figsize=(4,4))\n",
    "velmin = -0.3\n",
    "velmax = 0.3\n",
    "aicheader['BMAJ'] = cube.header['BMAJ']\n",
    "aicheader['BMIN'] = cube.header['BMIN']\n",
    "aicheader['BPA'] = cube.header['BPA']\n",
    "ax = plot_aplpy_subfig(fits.PrimaryHDU(params1g[1]-vlsr_irs1, aicheader),fig, (1,1,1), 'linear', velmin, velmax, 'RdYlBu_r')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
